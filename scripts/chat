#!/usr/bin/env -S pkgx +llama.cpp +huggingface-cli=0.19.4 +gum bash

set -eo pipefail

case "$1" in
--help|-h|/\?)
	gum format <<-EoMD
		usage: $0 [--llama2|--dolphin-mistral|--llama2-13b]

		to just take our (evolving) suggested defaults do: $0 kk

		> [llama2](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF)
		> [dolphin-mistral](https://huggingface.co/ehartford/dolphin-2.1-mistral-7b)
		EoMD
	exit
esac

get_model_choice() {
	case "$1" in
	--dolphin-mistral)
		echo 'Dolphin 2.1 Mistral 7B Q4';;
	--llama2)
		echo 'LLaMA 2 7B Q4';;
	kk)
		echo 'LLaMA 2 7B Q4';;
	--llama2-13b)
		echo 'LLaMA 2 13B Q4';;
	*)
		gum choose \
			--header '7B models are about 4 GiB, 13B are about 8 GiB' \
			'LLaMA 2 7B Q4' \
			'Dolphin 2.1 Mistral 7B Q4' \
			'LLaMA 2 13B Q4'
	esac
}

get_hf_args() {
	case "$1" in
	Dolphin\ 2.1\ Mistral\ 7B\ Q4)
		echo \
			TheBloke/dolphin-2.1-mistral-7B-GGUF \
			dolphin-2.1-mistral-7b.Q4_K_M.gguf;;
	LLaMA\ 2\ 7B\ Q4)
		echo \
			TheBloke/Llama-2-7B-Chat-GGUF \
			llama-2-7b-chat.Q4_K_S.gguf;;
	LLaMA\ 2\ 13B\ Q4)
		echo TheBloke/Llama-2-13B-chat-GGUF llama-2-13b-chat.Q4_K_S.gguf;;
	*)
		echo 'unexpected error' >&2
		exit 1
	esac
}

get_llama_cpp_args() {
	case $1 in
	LLaMA*)
		# some tips from here: https://github.com/ggerganov/llama.cpp/discussions/2201#discussioncomment-6430917
		# context size is mostly thought to be 4096 in various places
		echo '--ctx-size 4096 --keep -1 --batch-size 7 --instruct'
		;;
	Dolphin*)
		echo '--chatml --n-predict -1';;
	*)
		echo 'unexpected error' >&2
		exit 1
	esac
}

choice="$(get_model_choice $1)"

hf_args=($(get_hf_args "$choice"))
llamacpp_args=($(get_llama_cpp_args "$choice"))

model="$(huggingface-cli download "${hf_args[@]}")"

D="$(which llama.cpp)"
D="$(dirname $D)"
D="$(cd "$D/.." && pwd)"

exec "$D/bin/llama.cpp" \
	--model "$model" \
	"${llamacpp_args[@]}" \
	--repeat_penalty 1.1
	--color \
	--interactive
