#!/usr/bin/env -S pkgx +llama.cpp>=1833 +huggingface-cli=0.19.4 +gum bash

set -eo pipefail

case "$1" in
--help|-h|/\?)
	gum format <<-EoMD
		usage: \`mash ai chat [model]\`

		models:
		\`--llama2\`
		\`--llama2-13b\`
		\`--dolphin-mistral\`
		\`--openhermes-mistral\`

		leave blank for a TUI chooser.

		\`mash ai chat --\` for chefâ€™s choice.

		> [llama2](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF)
		> [dolphin-mistral](https://huggingface.co/ehartford/dolphin-2.1-mistral-7b)
		EoMD
	exit
esac

get_model_choice() {
	case "$1" in
	--dolphin-mistral)
		echo 'Dolphin 2.1 Mistral 7B Q4';;
	--llama2|--)
		echo 'LLaMA 2 7B Q4';;
	--llama2-13b)
		echo 'LLaMA 2 13B Q4';;
	--openhermes-mistral)
		echo 'OpenHermes 2.5 Mistral 7B Q8';;
	*)
		gum choose \
			--header '7B models are about 4 GiB, 13B are about 8 GiB' \
			'LLaMA 2 7B Q4' \
			'Dolphin 2.1 Mistral 7B Q4' \
			'LLaMA 2 13B Q4' \
			'OpenHermes 2.5 Mistral 7B Q8'
	esac
}

get_hf_args() {
	case "$1" in
	Dolphin\ 2.1\ Mistral\ 7B\ Q4)
		echo \
			TheBloke/dolphin-2.1-mistral-7B-GGUF \
			dolphin-2.1-mistral-7b.Q4_K_M.gguf;;
	LLaMA\ 2\ 7B\ Q4)
		echo \
			TheBloke/Llama-2-7B-Chat-GGUF \
			llama-2-7b-chat.Q4_K_S.gguf;;
	LLaMA\ 2\ 13B\ Q4)
		echo TheBloke/Llama-2-13B-chat-GGUF llama-2-13b-chat.Q4_K_S.gguf;;
	OpenHermes\ 2.5\ Mistral\ 7B\ Q8)
		echo \
			TheBloke/OpenHermes-2.5-Mistral-7B-GGUF \
			openhermes-2.5-mistral-7b.Q8_0.gguf;;
	*)
		echo 'unexpected error' >&2
		exit 1
	esac
}

get_llama_cpp_args() {
	case $1 in
	LLaMA*)
		# some tips from here: https://github.com/ggerganov/llama.cpp/discussions/2201#discussioncomment-6430917
		# context size is mostly thought to be 4096 in various places
		echo '--ctx-size 4096 --keep -1 --batch-size 7 --instruct'
		;;
	Dolphin*|OpenHermes*)
		echo '--chatml --n-predict -1';;
	*)
		echo 'unexpected error' >&2
		exit 1
	esac
}

choice="$(get_model_choice $1)"

hf_args=($(get_hf_args "$choice"))
llamacpp_args=($(get_llama_cpp_args "$choice"))

model="$(huggingface-cli download "${hf_args[@]}")"

D="$(which llama.cpp)"
D="$(dirname $D)"
D="$(cd "$D/.." && pwd)"

exec "$D/bin/llama.cpp" \
	--model "$model" \
	"${llamacpp_args[@]}" \
	--repeat_penalty 1.1
	--color \
	--interactive
